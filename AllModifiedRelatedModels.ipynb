{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Private Training\n",
        "Student Name & Number =Sara Rezanezhad / 99101643\n",
        "Student Name & Number =Kimia Fakheri / *******\n"
      ],
      "metadata": {
        "id": "n1DOGobwaKEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from sklearn.utils import resample\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import pyplot\n"
      ],
      "metadata": {
        "id": "UTegUi_Wnttv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_mnist():\n",
        "    from keras.datasets import fashion_mnist\n",
        "\n",
        "    # load dataset\n",
        "    (trainX, trainY), (testX, testY) = fashion_mnist.load_data()\n",
        "    # reshape dataset to have a single channel\n",
        "    trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
        "    testX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
        "    # one hot encode target values\n",
        "    trainY = to_categorical(trainY)\n",
        "    testY = to_categorical(testY)\n",
        "\n",
        "    return (trainX, trainY), (testX, testY)\n",
        "# Number of target models\n",
        "NUM_TARGET = 1\n",
        "# Number of shadow models\n",
        "NUM_SHADOW = 2\n",
        "VERBOSE = 3\n",
        "\n",
        "\"\"\"\n",
        "    Samples the training and testing data to create multiple smaller datasets.\n",
        "\n",
        "    Args:\n",
        "        train_data (Tuple[np.ndarray, np.ndarray]): Original training data (X_train, y_train).\n",
        "        test_data (Tuple[np.ndarray, np.ndarray]): Original testing data (X_test, y_test).\n",
        "        num_sets (int): Number of smaller datasets to create.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Tuple[list, list], Tuple[list, list]]:\n",
        "            - Resampled training data ([X_train1, X_train2, ...], [y_train1, y_train2, ...])\n",
        "            - Resampled testing data ([X_test1, X_test2, ...], [y_test1, y_test2, ...])\n",
        "    \"\"\"\n",
        "def sample_data(train_data, test_data, num_sets):\n",
        "    (x_train, y_train), (x_test, y_test) = train_data, test_data\n",
        "    x_train2, y_train2 = [], []\n",
        "    x_test2, y_test2 = [], []\n",
        "    for i in range(num_sets):\n",
        "        x_temp, y_temp = resample(x_train, y_train, n_samples=3000, random_state=0)\n",
        "        x_train2.append(x_temp)\n",
        "        y_train2.append(y_temp)\n",
        "        x_temp, y_temp = resample(x_test, y_test, n_samples=2000, random_state=0)\n",
        "        x_test2.append(x_temp)\n",
        "        y_test2.append(y_temp)\n",
        "    return (x_train2, y_train2), (x_test2, y_test2)\n",
        "\n",
        "    \"\"\"\n",
        "    Generates a dataset for the attack model by extracting predictions from the target and shadow models.\n",
        "\n",
        "    Args:\n",
        "        models (list): List of trained target and shadow models.\n",
        "        train_data (Tuple[np.ndarray, np.ndarray]): Original training data (X_train, y_train).\n",
        "        test_data (Tuple[np.ndarray, np.ndarray]): Original testing data (X_test, y_test).\n",
        "        num_models (int): Number of target and shadow models.\n",
        "        data_size (int): Size of the attack dataset.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[list, list]:\n",
        "            - X_data: List of model predictions, where each inner list represents a class.\n",
        "            - y_data: List of labels, where 1 represents a target model and 0 represents a shadow model.\n",
        "    \"\"\"\n",
        "def attack_dataset(models, train_data, test_data, num_models, data_size):\n",
        "    # generate dataset for the attack model\n",
        "    (x_train, y_train), (x_test, y_test) = train_data, test_data\n",
        "    # set number of classes for the attack model\n",
        "    num_classes = 10\n",
        "    x_data, y_data = [[] for _ in range(num_classes)], [[] for _ in range(num_classes)]\n",
        "    for i in range(num_models):\n",
        "\n",
        "        x_temp, y_temp = resample(x_train[i], y_train[i], n_samples=data_size, random_state=0)\n",
        "        for j in range(data_size):\n",
        "            y_idx = np.argmax(y_temp[j])\n",
        "            x_data[y_idx].append(models[i].predict(x_temp[j:j+1], verbose=0)[0])\n",
        "            y_data[y_idx].append(1)\n",
        "        x_temp, y_temp = resample(x_test[i], y_test[i], n_samples=data_size, random_state=0)\n",
        "        for j in range(data_size):\n",
        "            y_idx = np.argmax(y_temp[j])\n",
        "            x_data[y_idx].append(models[i].predict(x_temp[j:j+1], verbose=0)[0])\n",
        "            y_data[y_idx].append(0)\n",
        "\n",
        "    return x_data, y_data\n",
        "\n",
        "    \"\"\"\n",
        "    Defines a Fully Connected Neural Network (FCNN) model for the Fashion-MNIST classification task.\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Model: The FCNN model.\n",
        "    \"\"\"\n",
        "def FCNN():\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),  # Add dropout with 50% rate\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def get_trained_keras_models(keras_model, train_data, test_data, num_models):\n",
        "    (x_train, y_train), (x_test, y_test) = train_data, test_data\n",
        "    models = []\n",
        "    for i in range(num_models):\n",
        "        models.append(tf.keras.models.clone_model(keras_model))\n",
        "        rms = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "        models[i].compile(loss='categorical_crossentropy', optimizer=rms, metrics=['accuracy'])\n",
        "        models[i].fit(x_train[i], y_train[i], batch_size=32, epochs=5, verbose=VERBOSE, shuffle=True)\n",
        "        score = models[i].evaluate(x_test[i], y_test[i], verbose=VERBOSE)\n",
        "        print('\\n', 'Model ', i, ' test accuracy:', score[1])\n",
        "    return models\n",
        "\n",
        "def get_trained_svm_models(train_data, test_data, num_models=1):\n",
        "    from sklearn import svm\n",
        "    (x_train, y_train), (x_test, y_test) = train_data, test_data\n",
        "    models = []\n",
        "    for i in range(num_models):\n",
        "        print('Training svm model : ', i)\n",
        "        models.append(svm.SVC(gamma='scale', kernel='linear', verbose=VERBOSE))\n",
        "        models[i].fit(x_train[i], y_train[i])\n",
        "        score = models[i].score(x_test[i], y_test[i])\n",
        "        print('SVM model ', i, 'score : ', score)\n",
        "    return models\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KORi4b9lsfCm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    # load the pre-shuffled train and test data\n",
        "    (x_train, y_train), (x_test, y_test) = load_mnist()\n",
        "\n",
        "    # split the data for each model\n",
        "    target_train = (x_train[:3000*NUM_TARGET], y_train[:3000*NUM_TARGET])\n",
        "    target_test = (x_test[:2000*NUM_TARGET], y_test[:2000*NUM_TARGET])\n",
        "    target_train_data, target_test_data = sample_data(target_train, target_test, NUM_TARGET)\n",
        "\n",
        "    shadow_train = (x_train[3000*NUM_TARGET:], y_train[3000*NUM_TARGET:])\n",
        "    shadow_test = (x_test[2000*NUM_TARGET:], y_test[2000*NUM_TARGET:])\n",
        "    shadow_train_data, shadow_test_data = sample_data(shadow_train, shadow_test, NUM_SHADOW)\n",
        "\n",
        "    CNN_model = FCNN()\n",
        "\n",
        "    # Q5\n",
        "    target_models = get_trained_keras_models(CNN_model, target_train_data, target_test_data, NUM_TARGET)\n",
        "    # Q6\n",
        "    shadow_models = get_trained_keras_models(CNN_model, shadow_train_data, shadow_test_data, NUM_SHADOW)\n",
        "\n",
        "    # get train data for the attack model\n",
        "    attack_train = attack_dataset(shadow_models, shadow_train_data, shadow_test_data, NUM_SHADOW, 2000)\n",
        "    # Q7\n",
        "    attack_test = attack_dataset(target_models, target_train_data, target_test_data, NUM_TARGET, 2000)\n",
        "\n",
        "    # training the attack model\n",
        "    attack_model = get_trained_svm_models(attack_train, attack_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsiDdc-trqdv",
        "outputId": "fdd5a53f-07e4-45b8-9d14-5813a484984c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 2s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 1s 0us/step\n",
            "Epoch 1/5\n",
            "Epoch 2/5\n",
            "Epoch 3/5\n",
            "Epoch 4/5\n",
            "Epoch 5/5\n",
            "\n",
            " Model  0  test accuracy: 0.5245000123977661\n",
            "Epoch 1/5\n",
            "Epoch 2/5\n",
            "Epoch 3/5\n",
            "Epoch 4/5\n",
            "Epoch 5/5\n",
            "\n",
            " Model  0  test accuracy: 0.5715000033378601\n",
            "Epoch 1/5\n",
            "Epoch 2/5\n",
            "Epoch 3/5\n",
            "Epoch 4/5\n",
            "Epoch 5/5\n",
            "\n",
            " Model  1  test accuracy: 0.6065000295639038\n",
            "Training svm model :  0\n",
            "[LibSVM]SVM model  0 score :  0.5\n"
          ]
        }
      ]
    }
  ]
}