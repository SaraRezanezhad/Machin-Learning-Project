{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Private Training\n",
        "Student Name & Number =Sara Rezanezhad / 99101643\n",
        "Student Name & Number =Kimia Fakheri / *******\n"
      ],
      "metadata": {
        "id": "n1DOGobwaKEE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import necessary packages"
      ],
      "metadata": {
        "id": "GBq1eJT2e6hg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6B6bVNcJ6f0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f9376dd-bf1d-445e-8ac0-ce028886cd9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opacus\n",
            "  Downloading opacus-1.4.1-py3-none-any.whl (226 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/226.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/226.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from opacus) (1.25.2)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from opacus) (2.3.0+cu121)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.10/dist-packages (from opacus) (1.11.4)\n",
            "Requirement already satisfied: opt-einsum>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from opacus) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0->opacus)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->opacus) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0->opacus) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, opacus\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 opacus-1.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install opacus\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "\n",
        "from torchvision.datasets import CIFAR10\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import CIFAR10\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "from opacus import PrivacyEngine"
      ],
      "metadata": {
        "id": "L2QSn6CG6uZ-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Provided Model"
      ],
      "metadata": {
        "id": "R8EjqozIelij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFAR10Classifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CIFAR10Classifier, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 16, 3, 1)\n",
        "    self.conv2 = nn.Conv2d(16, 32, 3, 1)\n",
        "    self.dropout1 = nn.Dropout2d(0.25)\n",
        "    self.dropout2 = nn.Dropout2d(0.5)\n",
        "    self.fc1 = nn.Linear(6272, 64)\n",
        "    self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(x)\n",
        "    x = F.max_pool2d(x, 2)\n",
        "    x = self.dropout1(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.dropout2(x)\n",
        "    x = self.fc2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Po77HyT4eoqI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline Model (Simulation Question 4)\n",
        "This code trains the CIFAR10Classifier model on 80% of the CIFAR-10 training data and uses the remaining 20% as the validation set. The model is trained for 20 epochs, and the training loss, validation loss, and validation accuracy are printed at the end of each epoch.\n"
      ],
      "metadata": {
        "id": "awe4AZoehs3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# Load CIFAR-10 dataset (assuming you've already done this)\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Split the dataset into training (80%) and validation (20%) sets\n",
        "train_size = int(0.8 * len(trainset))\n",
        "val_size = len(trainset) - train_size\n",
        "trainset, valset = torch.utils.data.random_split(trainset, [train_size, val_size])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Initialize the model and move it to the device\n",
        "model = CIFAR10Classifier().to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    val_loss /= len(val_loader)\n",
        "    val_accuracy = 100 * correct / total\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "torch.save(model.state_dict(), 'BaselineModel.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzeVM986htK9",
        "outputId": "9c1d351b-6fe5-4a12-eaf6-8801c5756680"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Epoch [1/10], Train Loss: 1.7640, Val Loss: 1.4198, Val Accuracy: 50.92%\n",
            "Epoch [2/10], Train Loss: 1.5207, Val Loss: 1.2703, Val Accuracy: 57.32%\n",
            "Epoch [3/10], Train Loss: 1.4126, Val Loss: 1.1672, Val Accuracy: 59.51%\n",
            "Epoch [4/10], Train Loss: 1.3383, Val Loss: 1.1189, Val Accuracy: 61.36%\n",
            "Epoch [5/10], Train Loss: 1.2799, Val Loss: 1.0770, Val Accuracy: 62.80%\n",
            "Epoch [6/10], Train Loss: 1.2345, Val Loss: 1.0694, Val Accuracy: 62.75%\n",
            "Epoch [7/10], Train Loss: 1.1960, Val Loss: 1.0383, Val Accuracy: 64.15%\n",
            "Epoch [8/10], Train Loss: 1.1653, Val Loss: 1.0301, Val Accuracy: 63.87%\n",
            "Epoch [9/10], Train Loss: 1.1321, Val Loss: 1.0177, Val Accuracy: 65.03%\n",
            "Epoch [10/10], Train Loss: 1.1101, Val Loss: 1.0064, Val Accuracy: 65.12%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modified model(Simulation Question 5)"
      ],
      "metadata": {
        "id": "fgqNd88mmXNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Split the training data into training and validation sets\n",
        "num_train = len(trainset)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "train_indices = indices[:int(0.8 * num_train)]\n",
        "val_indices = indices[int(0.8 * num_train):]\n",
        "train_subset = Subset(trainset, train_indices)\n",
        "val_subset = Subset(trainset, val_indices)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_subset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "# Initialize the model and move it to the device\n",
        "model2 = CIFAR10Classifier().to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model2.parameters(), lr=0.001)\n",
        "\n",
        "# Initialize the Privacy Engine\n",
        "privacy_engine = PrivacyEngine()\n",
        "\n",
        "# Make the model, optimizer, and data loader private\n",
        "model2, optimizer, train_loader = privacy_engine.make_private(\n",
        "    module=model2,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_loader,\n",
        "    noise_multiplier=0.1,\n",
        "    max_grad_norm=1.0\n",
        ")\n",
        "\n",
        "#privacy_engine.attach(optimizer)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 15\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model2.train()\n",
        "    train_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model2(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "\n",
        "    # Validation model\n",
        "    model2.eval()\n",
        "    val_loss = 0.0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model2(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    val_loss /= len(val_loader)\n",
        "    val_accuracy = (correct * 100) / total\n",
        "\n",
        "    # Calculate the privacy budget spent so far\n",
        "    epsilon = privacy_engine.get_epsilon(delta=1e-5)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%, Epsilon: {epsilon:.2f}')\n",
        "\n",
        "torch.save(model2.state_dict(), 'ModifiedModel.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2tpWMEPJGLx",
        "outputId": "a29f2a5b-fb20-421c-f1e8-add6c8e7c355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/12], Train Loss: 2.0898, Val Loss: 1.8559, Val Accuracy: 34.40%, Epsilon: 64.65\n",
            "Epoch [2/12], Train Loss: 1.9669, Val Loss: 1.7622, Val Accuracy: 37.67%, Epsilon: 85.01\n",
            "Epoch [3/12], Train Loss: 1.9361, Val Loss: 1.7044, Val Accuracy: 40.65%, Epsilon: 101.70\n",
            "Epoch [4/12], Train Loss: 1.9203, Val Loss: 1.6565, Val Accuracy: 42.52%, Epsilon: 116.54\n",
            "Epoch [5/12], Train Loss: 1.9130, Val Loss: 1.6617, Val Accuracy: 43.34%, Epsilon: 130.23\n",
            "Epoch [6/12], Train Loss: 1.9132, Val Loss: 1.6240, Val Accuracy: 44.72%, Epsilon: 143.11\n",
            "Epoch [7/12], Train Loss: 1.9212, Val Loss: 1.6018, Val Accuracy: 45.52%, Epsilon: 155.38\n",
            "Epoch [8/12], Train Loss: 1.9092, Val Loss: 1.5699, Val Accuracy: 46.80%, Epsilon: 167.16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the smallest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/12], Train Loss: 1.9115, Val Loss: 1.6264, Val Accuracy: 46.35%, Epsilon: 178.56\n",
            "Epoch [10/12], Train Loss: 1.9118, Val Loss: 1.5770, Val Accuracy: 46.91%, Epsilon: 189.64\n",
            "Epoch [11/12], Train Loss: 1.9074, Val Loss: 1.5631, Val Accuracy: 47.10%, Epsilon: 200.44\n",
            "Epoch [12/12], Train Loss: 1.9029, Val Loss: 1.5963, Val Accuracy: 48.08%, Epsilon: 211.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the two attacker models based on MIA techniques (Simulation Question 6)"
      ],
      "metadata": {
        "id": "dkM45L765haw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attacker for Modifide Model"
      ],
      "metadata": {
        "id": "NHMVYKiScD2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the modified model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "modified_model = CIFAR10Classifier().to(device)\n",
        "state_dict = torch.load('/content/ModifiedModel.pth', map_location='cpu')\n",
        "new_state_dict = {k.replace('_module.', ''): v for k, v in state_dict.items()}  # Adjust keys if needed\n",
        "\n",
        "# Load the modified state_dict into the model\n",
        "modified_model.load_state_dict(new_state_dict)\n",
        "# Get the output size of the baseline model\n",
        "with torch.no_grad():\n",
        "    example_input = torch.randn(1, 3, 32, 32).to(device)\n",
        "    modified_output = modified_model(example_input)\n",
        "    attacker_modified_input_size = modified_output.size(1)\n",
        "\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Load the training and test data\n",
        "train_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Split the training data into 80% seen data and 20% unseen data\n",
        "train_seen, train_unseen, _, _ = train_test_split(train_data, train_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create PyTorch DataLoaders for the seen and unseen data\n",
        "train_seen_loader = DataLoader(train_seen, batch_size=32, shuffle=True)\n",
        "train_unseen_loader = DataLoader(train_unseen, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the input and output sizes for the shadow and attacker models\n",
        "input_size = 3 * 32 * 32  # Assuming CIFAR-10 images with 3 channels and 32x32 resolution\n",
        "output_size = 10  # 10 classes in CIFAR-10\n",
        "hidden_size = 128\n",
        "\n",
        "# Define the shadow model architecture\n",
        "shadow_model = nn.Sequential(\n",
        "    nn.Linear(input_size, hidden_size),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden_size, output_size)\n",
        ")\n",
        "\n",
        "# Train the shadow model on the seen data\n",
        "optimizer = optim.Adam(shadow_model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "num_epochs =10\n",
        "for epoch in range(num_epochs):\n",
        "    for data, target in train_seen_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = shadow_model(data.view(data.size(0), -1))  # Flatten the input data\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Train the attacker model on the seen and unseen data\n",
        "attacker_model = nn.Sequential(\n",
        "    nn.Linear(input_size, hidden_size),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden_size, 10)  # Output 2 classes: member or non-member\n",
        ")\n",
        "\n",
        "optimizer = optim.Adam(attacker_model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for data, target in train_seen_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = attacker_model(data.view(data.size(0), -1))\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    for data, target in train_unseen_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = attacker_model(data.view(data.size(0), -1))\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch [{epoch}/{num_epochs}] Loss:{loss:.4f}\")\n",
        "\n",
        "\n",
        "def calculate_mia_accuracy(attacker_model, train_seen_loader, train_unseen_loader, test_loader):\n",
        "    correct_seen = 0\n",
        "    total_seen = 0\n",
        "    correct_unseen = 0\n",
        "    total_unseen = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in train_seen_loader:\n",
        "            output = attacker_model(data.view(data.size(0), -1))\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct_seen += pred.eq(target.view_as(pred)).sum().item()\n",
        "            total_seen += data.size(0)\n",
        "\n",
        "        for data, target in train_unseen_loader:\n",
        "            output = attacker_model(data.view(data.size(0), -1))\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct_unseen += pred.eq(target.view_as(pred)).sum().item()\n",
        "            total_unseen += data.size(0)\n",
        "\n",
        "    seen_accuracy = correct_seen / total_seen\n",
        "    unseen_accuracy = correct_unseen / total_unseen\n",
        "    mia_accuracy = (seen_accuracy + unseen_accuracy) / 2\n",
        "\n",
        "    return mia_accuracy\n",
        "\n",
        "# Calculate the MIA accuracy\n",
        "mia_accuracy = calculate_mia_accuracy(attacker_model, train_seen_loader, train_unseen_loader, test_loader)\n",
        "print(f\"MIA Accuracy: {mia_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wv3xyeWQr3ka",
        "outputId": "4be24405-b3eb-4adb-a185-a08274d9a643"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch [0/10] Loss:1.0638\n",
            "Epoch [1/10] Loss:2.1377\n",
            "Epoch [2/10] Loss:1.2463\n",
            "Epoch [3/10] Loss:0.8980\n",
            "Epoch [4/10] Loss:1.1552\n",
            "Epoch [5/10] Loss:0.8915\n",
            "Epoch [6/10] Loss:1.0408\n",
            "Epoch [7/10] Loss:1.2874\n",
            "Epoch [8/10] Loss:0.9881\n",
            "Epoch [9/10] Loss:0.8388\n",
            "MIA Accuracy: 0.6757\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attacker for BaseLine"
      ],
      "metadata": {
        "id": "ki27Vk1HgHdE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQPsjDVSDBhM",
        "outputId": "a35413c3-c203-456f-96f5-0c3a7671854b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Epoch [1/3], Train Loss: 1.7808, Val Loss: 1.4418, Val Accuracy: 49.36%\n",
            "Epoch [2/3], Train Loss: 1.5333, Val Loss: 1.3025, Val Accuracy: 54.03%\n",
            "Epoch [3/3], Train Loss: 1.4460, Val Loss: 1.2170, Val Accuracy: 58.14%\n",
            "Epoch [1/3], Train Loss: 1.7613, Val Loss: 1.4022, Val Accuracy: 50.67%\n",
            "Epoch [2/3], Train Loss: 1.5051, Val Loss: 1.2442, Val Accuracy: 56.45%\n",
            "Epoch [3/3], Train Loss: 1.3935, Val Loss: 1.1691, Val Accuracy: 59.91%\n",
            "Epoch [1/3], Train Loss: 1.7684, Val Loss: 1.3919, Val Accuracy: 50.82%\n",
            "Epoch [2/3], Train Loss: 1.5011, Val Loss: 1.2453, Val Accuracy: 55.63%\n",
            "Epoch [3/3], Train Loss: 1.3918, Val Loss: 1.1591, Val Accuracy: 59.94%\n",
            "Epoch [1/3], Train Loss: 1.7851, Val Loss: 1.4396, Val Accuracy: 49.76%\n",
            "Epoch [2/3], Train Loss: 1.5516, Val Loss: 1.2998, Val Accuracy: 55.13%\n",
            "Epoch [3/3], Train Loss: 1.4457, Val Loss: 1.2099, Val Accuracy: 58.71%\n",
            "Epoch [1/3], Train Loss: 1.7656, Val Loss: 1.4025, Val Accuracy: 51.14%\n",
            "Epoch [2/3], Train Loss: 1.5368, Val Loss: 1.2715, Val Accuracy: 55.23%\n",
            "Epoch [3/3], Train Loss: 1.4413, Val Loss: 1.2188, Val Accuracy: 58.19%\n"
          ]
        }
      ],
      "source": [
        "from torchvision.datasets import CIFAR10\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Train multiple shadow models\n",
        "num_shadow_models = 5\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load CIFAR-10 dataset (assuming you've already done this)\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Split the dataset into training (80%) and validation (20%) sets\n",
        "train_size = int(0.8 * len(trainset))\n",
        "val_size = len(trainset) - train_size\n",
        "trainset, valset = torch.utils.data.random_split(trainset, [train_size, val_size])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=False)\n",
        "# Define the loss function and optimizer\n",
        "\n",
        "\n",
        "for i in range(num_shadow_models):\n",
        "    shadow_model = CIFAR10Classifier().to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(shadow_model.parameters(), lr=0.001)\n",
        "    for epoch in range(3):\n",
        "        # Training\n",
        "        shadow_model.train()\n",
        "        train_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = shadow_model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        shadow_model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = shadow_model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "        val_loss /= len(val_loader)\n",
        "        val_accuracy = 100 * correct / total\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{3}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "    # Save the trained shadow model\n",
        "    torch.save(shadow_model.state_dict(), f'ShadowModel_{i}.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0LkMeVbs-5Y",
        "outputId": "bb1280b4-4e01-41df-9980-2b171b0c3eda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Attacker Model Accuracy (Unseen Data): 0.9850\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import ConcatDataset\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the attacker model architecture\n",
        "class AttackerModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(AttackerModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Load data\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "seen_size = int(0.8 * len(trainset))\n",
        "unseen_size = len(trainset) - seen_size\n",
        "seen_trainset, unseen_trainset = torch.utils.data.random_split(trainset, [seen_size, unseen_size])\n",
        "\n",
        "seen_loader = torch.utils.data.DataLoader(seen_trainset, batch_size=64, shuffle=True)\n",
        "unseen_train_loader = torch.utils.data.DataLoader(ConcatDataset([unseen_trainset, testset]), batch_size=64, shuffle=False)\n",
        "\n",
        "baseline_model = CIFAR10Classifier().to(device)\n",
        "baseline_model.load_state_dict(torch.load('/content/BaselineModel.pth'))\n",
        "baseline_model.eval()\n",
        "# Get the output size of the baseline model\n",
        "with torch.no_grad():\n",
        "    example_input = torch.randn(1, 3, 32, 32).to(device)\n",
        "    baseline_output = baseline_model(example_input)\n",
        "    attacker_baseline_input_size = baseline_output.size(1)\n",
        "\n",
        "# Initialize the attacker model\n",
        "num_shadow_models = 5\n",
        "attacker_model = AttackerModel(attacker_baseline_input_size * num_shadow_models, 128, 1).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(attacker_model.parameters(), lr=0.001)\n",
        "num_epochs = 5  # Increased epochs\n",
        "\n",
        "# Initialize lists to store accuracies\n",
        "train_accuracies_seen = []\n",
        "train_accuracies_unseen = []\n",
        "# Train multiple shadow models\n",
        "num_shadow_models = 5\n",
        "shadow_models = []\n",
        "for i in range(num_shadow_models):\n",
        "    shadow_model = CIFAR10Classifier().to(device)\n",
        "    shadow_model.load_state_dict(torch.load(f'ShadowModel_{i}.pth'))\n",
        "    shadow_model.eval()\n",
        "    shadow_models.append(shadow_model)\n",
        "# Train the attacker models using shadow models\n",
        "for shadow_model in shadow_models:\n",
        "  for epoch in range(num_epochs):\n",
        "  # Evaluate the attacker model on seen data\n",
        "        correct_seen, total_seen = 0, 0\n",
        "        for seen_data, _ in seen_loader:\n",
        "            seen_output = torch.cat([model(seen_data.to(device)) for model in shadow_models], dim=1)\n",
        "            attacker_pred = attacker_model(seen_output)\n",
        "            predicted_labels = (attacker_pred > 0.5).float()\n",
        "            correct_seen += (predicted_labels == 1).sum().item()\n",
        "            total_seen += seen_data.size(0)\n",
        "        accuracy_seen = correct_seen / total_seen\n",
        "        train_accuracies_seen.append(accuracy_seen)\n",
        "\n",
        "        # Evaluate the attacker model on unseen data\n",
        "        correct_unseen, total_unseen = 0, 0\n",
        "        for unseen_data, _ in unseen_train_loader:\n",
        "            unseen_output = torch.cat([model(unseen_data.to(device)) for model in shadow_models], dim=1)\n",
        "            attacker_pred = attacker_model(unseen_output)\n",
        "            predicted_labels = (attacker_pred > 0.5).float()\n",
        "            correct_unseen += (predicted_labels == 0).sum().item()\n",
        "            total_unseen += unseen_data.size(0)\n",
        "        accuracy_unseen = correct_unseen / total_unseen\n",
        "        train_accuracies_unseen.append(accuracy_unseen)\n",
        "\n",
        "# Evaluate the attacker model\n",
        "attacker_model.eval()\n",
        "correct_seen, total_seen = 0, 0\n",
        "correct_unseen, total_unseen = 0, 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for seen_data, _ in seen_loader:\n",
        "        seen_output = torch.cat([model(seen_data.to(device)) for model in shadow_models], dim=1)\n",
        "        attacker_pred = attacker_model(seen_output)\n",
        "        predicted_labels = (attacker_pred > 0.5).float()\n",
        "        correct_seen += (predicted_labels == 1).sum().item()\n",
        "        total_seen += seen_data.size(0)\n",
        "\n",
        "    for unseen_data, _ in unseen_train_loader:\n",
        "        unseen_output = torch.cat([model(unseen_data.to(device)) for model in shadow_models], dim=1)\n",
        "        attacker_pred = attacker_model(unseen_output)\n",
        "        predicted_labels = (attacker_pred > 0.5).float()\n",
        "        correct_unseen += (predicted_labels == 0).sum().item()\n",
        "        total_unseen += unseen_data.size(0)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_seen = correct_seen / total_seen\n",
        "accuracy_unseen = correct_unseen / total_unseen\n",
        "\n",
        "print(f\"Attacker Model Accuracy (Unseen Data): {accuracy_unseen:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 Membership Inference Attack\n"
      ],
      "metadata": {
        "id": "o5Gb1gR5hMAd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First Model"
      ],
      "metadata": {
        "id": "jVhngFRrhiSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Subset, DataLoader, TensorDataset\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the pre-trained private model\n",
        "model = CIFAR10Classifier()\n",
        "state_dict = torch.load(\"model_state_dict.pth\", map_location=device)\n",
        "new_state_dict = {key.replace('_module.', ''): value for key, value in state_dict.items()}\n",
        "model.load_state_dict(new_state_dict)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Load and preprocess the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "DATA_ROOT = '../cifar10'\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Load the indices from list.txt\n",
        "indices_file = 'list.txt'\n",
        "with open(indices_file, 'r') as f:\n",
        "    indices = [int(line.strip()) for line in f]\n",
        "\n",
        "full_train_dataset = CIFAR10(root=DATA_ROOT, train=True, download=True, transform=transform)\n",
        "test_dataset = CIFAR10(root=DATA_ROOT, train=False, download=True, transform=transform)\n",
        "\n",
        "train_indices_set = set(indices)\n",
        "all_indices = set(range(len(full_train_dataset)))\n",
        "other_indices = list(all_indices - train_indices_set)\n",
        "\n",
        "train_dataset = Subset(full_train_dataset, indices[:len(indices)//2])\n",
        "other_dataset = Subset(full_train_dataset, other_indices)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "other_loader = DataLoader(other_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Create labels\n",
        "train_labels = torch.ones(len(train_dataset)).to(device)\n",
        "other_labels = torch.zeros(len(other_dataset)).to(device)\n",
        "test_labels = torch.zeros(len(test_dataset)).to(device)\n",
        "\n",
        "def extract_features(model, dataloader):\n",
        "    model.eval()\n",
        "    features = []\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            inputs, _ = data\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            features.append(outputs)\n",
        "    return torch.cat(features).to(device)\n",
        "\n",
        "# Extract features from the private model\n",
        "train_features = extract_features(model, train_loader)\n",
        "other_features = extract_features(model, other_loader)\n",
        "test_features = extract_features(model, test_loader)\n",
        "\n",
        "# Combine features and labels\n",
        "combined_features = torch.cat((train_features, other_features, test_features))\n",
        "combined_labels = torch.cat((train_labels, other_labels, test_labels))\n",
        "\n",
        "# Create a new dataset and dataloader\n",
        "new_dataset = TensorDataset(combined_features, combined_labels)\n",
        "new_loader = DataLoader(new_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "#########################################################################################\n",
        "# Define the shadow model architecture\n",
        "shadow = nn.Sequential(\n",
        "    nn.Linear(input_size, hidden_size),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden_size, output_size)\n",
        ")\n",
        "\n",
        "\n",
        "attacker_model = nn.Sequential(\n",
        "    nn.Linear(input_size, hidden_size),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden_size, 10)  # Output 2 classes: member or non-member\n",
        ")\n",
        "attacker_optimizer = optim.Adam(attacker.parameters(), lr=0.001)\n",
        "attacker_loss_fn = nn.BCELoss()\n",
        "\n",
        "shadow_optimizer = optim.Adam(shadow.parameters(), lr=0.001)\n",
        "shadow_loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "attacker.to(device)\n",
        "shadow.to(device)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    attacker.train()\n",
        "    shadow.train()\n",
        "\n",
        "    for features, labels in attacker_train_loader:\n",
        "        features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "        # Train the attacker model\n",
        "        attacker_optimizer.zero_grad()\n",
        "        outputs = attacker(features).squeeze()\n",
        "        loss = attacker_loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        attacker_optimizer.step()\n",
        "\n",
        "        # Generate shadow model's predictions\n",
        "        shadow_optimizer.zero_grad()\n",
        "        shadow_outputs = shadow(features)\n",
        "        shadow_loss = shadow_loss_fn(shadow_outputs, labels)\n",
        "        shadow_loss.backward()\n",
        "        shadow_optimizer.step()\n",
        "\n",
        "\n",
        "# Evaluate the attacker model\n",
        "attacker.eval()\n",
        "all_labels = []\n",
        "all_predicted = []\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for features, labels in new_loader:\n",
        "        features, labels = features.to(device), labels.to(device)\n",
        "        outputs = attacker(features).squeeze()\n",
        "        predicted = (outputs > 0.5).float()\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_predicted.extend(predicted.cpu().numpy())\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Training Accuracy: {accuracy:.4f}')\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_predicted)\n",
        "precision = precision_score(all_labels, all_predicted)\n",
        "recall = recall_score(all_labels, all_predicted)\n",
        "f1 = f1_score(all_labels, all_predicted)\n",
        "\n",
        "print(f'Confusion Matrix:\\n{cm}')\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "fiNIwgVkOX1g",
        "outputId": "73635f36-cc0e-480a-fa8e-9eb40cc0ddec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'list.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-94157283076f>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Load the indices from list.txt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mindices_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'list.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'list.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### second Model"
      ],
      "metadata": {
        "id": "hjs2XOizhuSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Subset, DataLoader, TensorDataset\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = CIFAR10Classifier()\n",
        "state_dict = torch.load(\"model_state_dict.pth\", map_location=device)\n",
        "new_state_dict = {key.replace('_module.', ''): value for key, value in state_dict.items()}\n",
        "model.load_state_dict(new_state_dict)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "DATA_ROOT = '../cifar10'\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Load the indices from list.txt\n",
        "indices_file = 'list.txt'\n",
        "with open(indices_file, 'r') as f:\n",
        "    indices = [int(line.strip()) for line in f]\n",
        "\n",
        "full_train_dataset = CIFAR10(root=DATA_ROOT, train=True, download=True, transform=transform)\n",
        "test_dataset = CIFAR10(root=DATA_ROOT, train=False, download=True, transform=transform)\n",
        "\n",
        "train_indices_set = set(indices)\n",
        "all_indices = set(range(len(full_train_dataset)))\n",
        "other_indices = list(all_indices - train_indices_set)\n",
        "\n",
        "train_dataset = Subset(full_train_dataset, indices[:len(indices)//2])\n",
        "other_dataset = Subset(full_train_dataset, other_indices)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "other_loader = DataLoader(other_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Create labels\n",
        "train_labels = torch.ones(len(train_dataset)).to(device)\n",
        "other_labels = torch.zeros(len(other_dataset)).to(device)\n",
        "test_labels = torch.zeros(len(test_dataset)).to(device)\n",
        "\n",
        "def extract_features(model, dataloader):\n",
        "    model.eval()\n",
        "    features = []\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            inputs, _ = data\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            features.append(outputs)\n",
        "    return torch.cat(features).to(device)\n",
        "\n",
        "train_features = extract_features(model, train_loader)\n",
        "other_features = extract_features(model, other_loader)\n",
        "test_features = extract_features(model, test_loader)\n",
        "\n",
        "# Train shadow models\n",
        "num_shadow_models = 5\n",
        "shadow_models = []\n",
        "for _ in range(num_shadow_models):\n",
        "    shadow_model = LogisticRegression(random_state=42)\n",
        "    shadow_model.fit(train_features.cpu(), train_labels.cpu())\n",
        "    shadow_models.append(shadow_model)\n",
        "\n",
        "# Membership inference attack\n",
        "attacker = LogisticRegression(random_state=42)\n",
        "all_features = torch.cat((train_features, other_features, test_features)).cpu()\n",
        "all_labels = torch.cat((train_labels, other_labels, test_labels)).cpu()\n",
        "train_data, val_data, train_labels, val_labels = train_test_split(all_features, all_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train attacker model\n",
        "attacker.fit(train_data, train_labels)\n",
        "\n",
        "# Evaluate attacker model\n",
        "attacker.eval()\n",
        "val_predicted = attacker.predict(val_data)\n",
        "accuracy = (val_labels == val_predicted).mean()\n",
        "cm = confusion_matrix(val_labels, val_predicted)\n",
        "precision = precision_score(val_labels, val_predicted)\n",
        "recall = recall_score(val_labels, val_predicted)\n",
        "f1 = f1_score(val_labels, val_predicted)\n",
        "\n",
        "print(f'Attacker Model Accuracy: {accuracy:.4f}')\n",
        "print(f'Confusion Matrix:\\n{cm}')\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "s1nlYDTgYx4Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}